{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea04a3f",
   "metadata": {},
   "source": [
    "## Extraction des données par Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de librairies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c98a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour scraper - depuis le site web TrustPilot - les avis clients d'une entreprise donnée et les sauvegarder dans un fichier CSV\n",
    "def scrape_save_reviews(url, start_page=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Definition :\n",
    "    - Scrapes customer reviews across multiple pages from a given company's TrustPilot URL.\n",
    "    - Saves the scraped reviews into a CSV file.\n",
    "\n",
    "    Parameters :\n",
    "    - url (str) : The TrustPilot URL of the company's page to scrape reviews from.\n",
    "    (e.g. : https://fr.trustpilot.com/review/boursobank.com , https://fr.trustpilot.com/review/fortuneo.fr, https://fr.trustpilot.com/review/hellobank.fr, https://fr.trustpilot.com/review/www.monabanq.com)\n",
    "    - start_page (int) : The page number to start scraping from. Default is 1.\n",
    "    \n",
    "    Returns :\n",
    "    - A Pandas DataFrame containing the scraped reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    # Récupération de la réponse HTTPS de la page d'une entreprise\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        # Création d'un objet BeautifulSoup à partir du contenu HTML récupéré\n",
    "        soup = bs(response.text, 'lxml')\n",
    "\n",
    "        # Récupération du nombre de pages\n",
    "        total_pages = int(soup.find('a', attrs={'name': 'pagination-button-last'}).text.strip())\n",
    "\n",
    "        # Création d'un dictionnaire avec différentes variables\n",
    "        data = {\n",
    "            'pseudo': [],\n",
    "            'publication_date': [],\n",
    "            'rating': [],\n",
    "            'title': [],\n",
    "            'review': [],\n",
    "            'experience_date': [],\n",
    "            'total_reviews': []\n",
    "            #'country': []\n",
    "        }\n",
    "        \n",
    "        # Boucle sur les pages\n",
    "        for page in range(start_page, total_pages + 1):\n",
    "\n",
    "            # Récupération de la réponse HTTPS de la n-ième page\n",
    "            response = requests.get(url + '?page=' + str(page))\n",
    "\n",
    "            if response.status_code == 200:\n",
    "\n",
    "                # Création d'un objet BeautifulSoup à partir du contenu HTML récupéré\n",
    "                soup = bs(response.text, 'lxml')\n",
    "\n",
    "                # Récupération des avis\n",
    "                reviews = soup.find_all('div', class_='styles_cardWrapper__kOLEb styles_show__qAseP')\n",
    "\n",
    "                # Boucle sur les avis\n",
    "                for review in reviews:\n",
    "        \n",
    "                    # Pseudo du client\n",
    "                    pseudo_element = review.find('span', class_='typography_heading-xs__osRhC typography_appearance-default__t8iAq')\n",
    "                    data['pseudo'].append(pseudo_element.text.strip() if pseudo_element else '')\n",
    "\n",
    "                    # Date de publication de l'avis du client\n",
    "                    publication_date_element = review.find('time')['datetime']\n",
    "                    data['publication_date'].append(publication_date_element.strip() if publication_date_element else '')\n",
    "\n",
    "                    # Note liée à l'avis du client\n",
    "                    rating_element = review.find('div', class_='styles_reviewHeader__PuHBd')['data-service-review-rating']\n",
    "                    data['rating'].append(rating_element.strip() if rating_element else '')\n",
    "                 \n",
    "                    # Titre de l'avis du client\n",
    "                    title_element = review.find('h2', class_='typography_heading-xs__osRhC typography_appearance-default__t8iAq')\n",
    "                    data['title'].append(title_element.text.strip() if title_element else '')\n",
    "                \n",
    "                    # Texte de l'avis du client\n",
    "                    review_element = review.find('p', class_='typography_body-l__v5JLj typography_appearance-default__t8iAq')\n",
    "                    data['review'].append(review_element.text.strip() if review_element else '')\n",
    "\n",
    "                    # Date de l'expérience du client\n",
    "                    experience_date_element = review.find('p', class_='typography_body-m__k2UI7 typography_appearance-default__t8iAq').find('span')\n",
    "                    data['experience_date'].append(experience_date_element.text.strip() if experience_date_element else '')\n",
    "                    \n",
    "                    # Nombre d'avis du client\n",
    "                    total_reviews_element = review.find('div', class_='styles_consumerExtraDetails__TylYM')['data-consumer-reviews-count']\n",
    "                    data['total_reviews'].append(total_reviews_element.strip() if total_reviews_element else '')\n",
    "                    \n",
    "                    # Pays du client\n",
    "                    #country_element = review.find('div', class_='typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua')\n",
    "                    #data['country'].append(country_element.text.strip() if country_element else '')\n",
    "                    \n",
    "            else:\n",
    "                print('Erreur lors de la récupération de la page', page, ': erreur', response.status_code)\n",
    "                break\n",
    "\n",
    "        # Création d'un DataFrame à partir du dictionnaire \"data\"\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Récupération du nom de la banque dans l'URL et Ajout d'une colonne \"bank\" dans le DataFrame\n",
    "        match = re.search(r'/review/(?:www\\.)?(.*?)\\.', url)\n",
    "        df['bank'] = match.group(1)\n",
    "\n",
    "        # Sauvegarde du DataFrame dans un fichier CSV\n",
    "        df.to_csv('../data/raw/scraped_reviews_' + match.group(1) + '_pages_' + str(start_page) + '_to_' + str(page-1) + '.csv', index=False)\n",
    "\n",
    "        return df\n",
    "  \n",
    "    else:\n",
    "        print('Erreur lors de la récupération de la 1re page', ': erreur', response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping des avis clients d'une entreprise donnée et Sauvegarde dans un fichier CSV\n",
    "\n",
    "# >>> ATTENTION ! <<< des fichiers CSV existent déjà dans \"data/raw/\"\n",
    "\n",
    "%%time\n",
    "#df = scrape_save_reviews('https://fr.trustpilot.com/review/www.monabanq.com', 1)\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour concaténer des fichiers CSV et les sauvegarder dans un fichier CSV unique\n",
    "def concat_save_csv_files(file_prefix):\n",
    "\n",
    "    \"\"\"\n",
    "    Definition :\n",
    "    - Concatenates CSV files.\n",
    "    - Saves the concatenated CSV files into a CSV file.\n",
    "\n",
    "    Parameters :\n",
    "    - file_prefix (str) : The prefix of the CSV file.\n",
    "    (e.g. : scraped_reviews)\n",
    "    \n",
    "    Returns :\n",
    "    - A Pandas DataFrame containing the concatenated CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Récupération des fichiers dont le nom comporte le préfixe et qui se termine par un nombre suivi de l'extension \".csv\"\n",
    "    files = [file for file in os.listdir('../data/raw/') if file.startswith(file_prefix) and re.search(r'\\d+\\.csv$', file)]\n",
    "\n",
    "    # Création d'une liste de DataFrames en lisant chaque fichier CSV\n",
    "    dfs = [pd.read_csv(os.path.join('../data/raw/', file)) for file in files]\n",
    "    \n",
    "    # Concaténation des DataFrames à partir de la liste de DataFrames \"dfs\"\n",
    "    dfs_concatenated = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Sauvegarde du DataFrame dans un fichier CSV\n",
    "    dfs_concatenated.to_csv('../data/raw/scraped_reviews_final.csv', index=False)\n",
    "    \n",
    "    return dfs_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des fichiers CSV et Sauvegarde dans un fichier CSV unique\n",
    "\n",
    "# >>> ATTENTION ! <<< un fichier CSV existe déjà dans \"data/raw/\"\n",
    "\n",
    "%%time\n",
    "#dfs_concatenated = concat_save_csv_files('scraped_reviews')\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "dfs_concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f235e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55198, 8)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_concatenated.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environnement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
